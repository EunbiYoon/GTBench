# ğŸ® PPO vs Fixed CFR on Tic-Tac-Toe

This project implements a **Proximal Policy Optimization (PPO)** agent
trained via self-play on Tic-Tac-Toe using OpenSpiel.\
The trained PPO agent is periodically evaluated against a fixed **CFR
(Counterfactual Regret Minimization)** opponent using Monte-Carlo
rollouts.

------------------------------------------------------------------------

## ğŸš€ Overview

-   ğŸ¤– **Training:** PPO with self-play (shared network for both
    players)
-   ğŸ¯ **Opponent:** Fixed CFR average policy
-   ğŸ“Š **Evaluation:** Monte-Carlo expected value (both seatings)
-   ğŸ“ **Logging:** CSV logs + learning curve plot

------------------------------------------------------------------------

## ğŸ“‚ Project Structure

    gtbench/
    â”‚
    â”œâ”€â”€ config.py          # âš™ï¸ Hyperparameters and experiment configuration
    â”œâ”€â”€ agents.py          # ğŸ§  Actor-Critic model + action sampling utilities
    â”œâ”€â”€ train.py           # ğŸ”„ Rollout collection + PPO update logic
    â”œâ”€â”€ cfr_opponent.py    # ğŸ§® CFR training + action sampling
    â”œâ”€â”€ eval.py            # ğŸ“ˆ Monte-Carlo evaluation vs fixed CFR
    â”œâ”€â”€ ppo.py             # â–¶ï¸ Main training script
    â””â”€â”€ README.md

------------------------------------------------------------------------

## ğŸ‹ï¸ How It Works

### 1ï¸âƒ£ PPO Training (Self-Play)

-   A single Actor-Critic network controls **both Player 0 and Player
    1**
-   Rollouts are collected on-policy
-   PPO clipped objective is used for updates
-   GAE (Generalized Advantage Estimation) stabilizes training

The model improves by playing against itself ğŸ².

------------------------------------------------------------------------

### 2ï¸âƒ£ Fixed CFR Opponent

Before PPO training begins:

-   A CFR solver runs for a fixed number of iterations
-   The average CFR policy is extracted
-   This policy remains fixed during PPO training

This provides a strong equilibrium-style baseline opponent ğŸ§©.

------------------------------------------------------------------------

### 3ï¸âƒ£ Evaluation

Evaluation is performed periodically:

-   PPO as Player 0 vs CFR as Player 1
-   PPO as Player 1 vs CFR as Player 0

Monte-Carlo rollouts estimate:

-   ğŸ“Œ **EV_P0**: Expected reward when PPO plays first
-   ğŸ“Œ **EV_P1**: Expected reward when PPO plays second

------------------------------------------------------------------------

## â–¶ï¸ Running the Experiment

From the project root directory:

``` bash
python ppo.py
```

The script will:

1.  ğŸ§® Build the fixed CFR opponent\
2.  ğŸ”„ Train PPO via self-play\
3.  ğŸ“Š Periodically evaluate against CFR\
4.  ğŸ’¾ Save:
    -   `ppo_log.csv`
    -   `ppo_learning_curve.png`

------------------------------------------------------------------------

## âš™ï¸ Key Hyperparameters

Edit `config.py` to modify:

-   ğŸ“‰ Learning rate
-   ğŸ” Rollout steps
-   ğŸ”„ Number of updates
-   ğŸ² Entropy coefficient
-   ğŸ§® CFR iterations
-   ğŸ“Š Evaluation frequency

------------------------------------------------------------------------

## ğŸ“¦ Requirements

-   Python 3.8+
-   PyTorch
-   OpenSpiel
-   NumPy
-   Matplotlib

Example installation:

``` bash
pip install torch numpy matplotlib
pip install open_spiel
```

------------------------------------------------------------------------

## ğŸ“ Notes

-   PPO uses **stochastic action sampling**, not greedy actions.
-   Tic-Tac-Toe is highly sensitive to small mistakes âš ï¸.
-   Training is fully self-play; evaluation uses a stronger fixed
    opponent.

------------------------------------------------------------------------

âœ¨ Happy experimenting!
